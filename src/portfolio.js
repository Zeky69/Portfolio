/* Change this file to get your personal Porfolio */

//Home Page
const greeting = {
  title: "Bonjour.",
  title2: "Zekeriya Akburak",
  logo_name: "codeky.fr",
  nickname: "Zeky69 / Zekeriya69",
  full_name: "Zekeriya Akburak",
  subTitle: "Étudiant Développeur informatique",
  resumeLink: "",
  mail: "mailto:zek.akburak@gmail.com",
};

const socialMediaLinks = {
  /* Your Social Media Link */
  github: "https://github.com/Zeky69",
  linkedin: "https://www.linkedin.com/in/zekeriya-akburak-87073b271/",
  gmail: "zek.akburak@gmail.com",
  gitlab: " ",
  facebook: " ",
  twitter: " ",
  instagram: " ",
};

const skills = {
  data: [
    {
      title: "Full Stack Development",
      fileName: "FullStackImg",
      skills: [
        "⚡ Creation de sites web front-end et back-end complets",
        "⚡ Developpement d'applications mobiles avec Swift et Kotlin",
        "⚡ Creation de back-end avec NodeJS, Express , MongoDB et MySQL",
      ],
      softwareSkills: [
        {
          skillName: "HTML5",
          fontAwesomeClassname: "simple-icons:html5",
          style: {
            color: "#E34F26",
          },
        },
        {
          skillName: "CSS3",
          fontAwesomeClassname: "fa-css3",
          style: {
            color: "#1572B6",
          },
        },
        {
          skillName: "JavaScript",
          fontAwesomeClassname: "simple-icons:javascript",
          style: {
            backgroundColor: "#FFFFFF",
            color: "#F7DF1E",
          },
        },

        {
          skillName: "Java",
          fontAwesomeClassname: "hugeicons:java",
          style: {
            color: "#f89820",
          },
        },
        {
          skillName: "Kotlin",
          fontAwesomeClassname: "simple-icons:kotlin",
          style: {
            color: "#5c79df",
          },
        },
        {
          skillName: "C",
          fontAwesomeClassname: "simple-icons:c",
          style: {
            color: "#E94E32",
          },
        },
        {
          skillName: "C++",
          fontAwesomeClassname: "simple-icons:cplusplus",
          style: {
            color: "#E94E32",
          },
        },
        {
          skillName: "Python",
          fontAwesomeClassname: "simple-icons:python",
          style: {
            color: "#3776AB",
          },
        },
        {
          skillName: "Swift",
          fontAwesomeClassname: "simple-icons:swift",
          style: {
            color: "#FA7343",
          },
        },
        {
          skillName: "ReactJS",
          fontAwesomeClassname: "simple-icons:react",
          style: {
            color: "#61DAFB",
          },
        },
        {
          skillName: "NodeJS",
          fontAwesomeClassname: "simple-icons:nodedotjs",
          style: {
            color: "#339933",
          },
        },
        {
          skillName: "MongoDB",
          fontAwesomeClassname: "simple-icons:mongodb",
          style: {
            color: "#439743",
          },
        },
        {
          skillName: "VueJS",
          fontAwesomeClassname: "simple-icons:vuedotjs",
          style: {
            color: "#3fb581",
          },
        },
        {
          skillName: "MySQL",
          fontAwesomeClassname: "simple-icons:mysql",
          style: {
            color: "#4479A1",
          },
        },

        {
          skillName: "Git",
          fontAwesomeClassname: "simple-icons:git",
          style: {
            color: "#E94E32",
          },
        },
      ],
    },
    {
      title: "Data Science & AI",
      fileName: "DataScienceImg",
      skills: [
        "⚡ Savoir faire des modèles de Machine Learning et Deep Learning",
        "⚡ Experience avec des bibliothèques comme TensorFlow, Keras et Scikit-Learn",
        "⚡ Analyse de données et visualisation avec Pandas, Matplotlib et Seaborn",
      ],
      softwareSkills: [
        {
          skillName: "Tensorflow",
          fontAwesomeClassname: "logos-tensorflow",
          style: {
            backgroundColor: "transparent",
          },
        },
        {
          skillName: "Keras",
          fontAwesomeClassname: "simple-icons:keras",
          style: {
            backgroundColor: "white",
            color: "#D00000",
          },
        },
        {
          skillName: "PyTorch",
          fontAwesomeClassname: "logos-pytorch",
          style: {
            backgroundColor: "transparent",
          },
        },
        {
          skillName: "Python",
          fontAwesomeClassname: "ion-logo-python",
          style: {
            backgroundColor: "transparent",
            color: "#3776AB",
          },
        },
        {
          skillName: "Jupyter",
          fontAwesomeClassname: "simple-icons:jupyter",
          style: {
            backgroundColor: "transparent",
            color: "#F37626",
          },
        },
        {
          skillName: "Scikit-Learn",
          fontAwesomeClassname: "simple-icons:scikitlearn",
          style: {
            backgroundColor: "transparent",
            color: "#F7931E",
          },
        },
        {
          skillName: "Pandas",
          fontAwesomeClassname: "simple-icons:pandas",
          style: {
            backgroundColor: "transparent",
            color: "#6E6E6E",
          },
        },
      ],
    },
  ],
};

const degrees = {
  degrees: [
    {
      title: "BUT informatique",
      subtitle: "IUT Nord Franche-Comté",
      logo_path: "iut.png",
      alt_name: "IUT Nord Franche-Comté",
      duration: "2022 - 2025",
      descriptions: ["⚡ J'étudie à l'IUT Nord Franche-Comté à Belfort"],
      website_link: "",
    },
    {
      title: "BACCALAURÉAT GÉNÉRAL",
      subtitle: "Mathématiques, Numérique et sciences informatiques",
      logo_path: "diplome.jpeg",
      alt_name: "Diplome Bachelor",
      duration: "2022",
      descriptions: ["⚡ J'ai étudié au Lycée Armand Peugeot à Valentigney"],
      website_link: "",
    },
  ],
};

const certifications = {
  certifications: [
    // {
    //   title: "AWS Developer Associate",
    //   subtitle: "Amazon Web Services",
    //   logo_path: "aws.png",
    //   certificate_link:
    //     "https://www.credly.com/badges/b12fbece-07f3-47f5-9dda-cd56f49e250d/public_url",
    //   alt_name: "AWS",
    //   color_code: "#ffc475",
    // },
    // {
    //   title: "AWS Certified Cloud Practioner",
    //   subtitle: "Amazon Web Services",
    //   logo_path: "aws.png",
    //   certificate_link:
    //     "https://www.credly.com/badges/8b2db191-702d-427c-91aa-0f926be447a6/public_url",
    //   alt_name: "AWS",
    //   color_code: "#ffc475",
    // },
    // {
    //   title: "Google Summer of Code 2019",
    //   subtitle: "Google / Sugar Labs",
    //   logo_path: "google_logo.png",
    //   certificate_link: " ",
    //   alt_name: "Google",
    //   color_code: "#ffc475",
    // },
    // {
    //   title: "Google Code-In Student",
    //   subtitle: "2014-2017",
    //   logo_path: "google_logo.png",
    //   certificate_link: " ",
    //   alt_name: "Google",
    //   color_code: "#1e70c1",
    // },
    // {
    //   title: "Google Summer of Code Mentor",
    //   subtitle: "2017 / 2018 / 2020",
    //   logo_path: "google_logo.png",
    //   certificate_link: " ",
    //   alt_name: "Google",
    //   color_code: "#ffbfae",
    // },
    // {
    //   title: "Google Code-In Mentor",
    //   subtitle: "2017 / 2018 / 2019",
    //   logo_path: "google_logo.png",
    //   certificate_link: " ",
    //   alt_name: "Google",
    //   color_code: "#b190b0",
    // },
    // {
    //   title: "Deep Learning Specialization",
    //   subtitle: "deeplearning.ai",
    //   logo_path: "deeplearning_ai_logo.png",
    //   certificate_link:
    //     "https://coursera.org/share/737a9587023c666b8e6cb303157aaeba",
    //   alt_name: "deeplearning.ai",
    //   color_code: "#47A048",
    // },
    // {
    //   title: "Sequence Models",
    //   subtitle: "deeplearning.ai",
    //   logo_path: "deeplearning_ai_logo.png",
    //   certificate_link:
    //     "https://www.coursera.org/account/accomplishments/verify/FM5AKEZA9NUY",
    //   alt_name: "deeplearning.ai",
    //   color_code: "#F6B808",
    // },
    // {
    //   title: "Convolutional Neural Networks",
    //   subtitle: "deeplearning.ai",
    //   logo_path: "deeplearning_ai_logo.png",
    //   certificate_link:
    //     "https://www.coursera.org/account/accomplishments/verify/U8BLDNUT9UUM",
    //   alt_name: "deeplearning.ai",
    //   color_code: "#2AAFED",
    // },
    // {
    //   title: "Structuring Machine Learning Projects",
    //   subtitle: "deeplearning.ai",
    //   logo_path: "deeplearning_ai_logo.png",
    //   certificate_link:
    //     "https://www.coursera.org/account/accomplishments/verify/YLC25SJQKH3Y",
    //   alt_name: "deeplearning.ai",
    //   color_code: "#E2405F",
    // },
    // {
    //   title: "Machine Learning",
    //   subtitle: "deeplearning.ai",
    //   logo_path: "stanford_logo.png",
    //   certificate_link:
    //     "https://www.coursera.org/account/accomplishments/records/72KY93DT82MP",
    //   alt_name: "Stanford University",
    //   color_code: "#8C151599",
    // },
    // {
    //   title: "Neural Networks and Deep Learning",
    //   subtitle: "deeplearning.ai",
    //   logo_path: "deeplearning_ai_logo.png",
    //   certificate_link:
    //     "https://www.coursera.org/account/accomplishments/records/25JXRB2RWHRX",
    //   alt_name: "Google",
    //   color_code: "#7A7A7A",
    // },
    // {
    //   title: "Improving Deep Neural Networks",
    //   subtitle: "deeplearning.ai",
    //   logo_path: "deeplearning_ai_logo.png",
    //   certificate_link:
    //     "https://www.coursera.org/account/accomplishments/records/PKR9M9LQ3JWC",
    //   alt_name: "Google",
    //   color_code: "#0C9D5899",
    // },
    // {
    //   title: "Android Developer Nanodegree",
    //   subtitle: "Part of Google India Scholarship Program",
    //   logo_path: "100.png",
    //   certificate_link: "https://graduation.udacity.com/confirm/HLE7K5V3",
    //   alt_name: "Google",
    //   color_code: "#C5E2EE",
    // },
    // {
    //   title: "InOut 4.0 Winner #2",
    //   subtitle: "2017",
    //   logo_path: "ino.png",
    //   certificate_link: " ",
    //   alt_name: "InOut",
    //   color_code: "#fffbf3",
    // },
  ],
};

// Experience Page
const experience = {
  title: "Experience",
  subtitle: "",
  description:
    "Voici un aperçu de mes expériences professionnelles et événementielles",
  header_image_path: "experience.svg",
  sections: [
    {
      title: "Expérience Professionnelle",
      experiences: [
        {
          title: "Stage en recherche et développement",
          company: "Femto-st",
          company_url: "https://www.femto-st.fr/fr",
          logo_path: "logo-femto.png",
          duration: "Avril 2024 - Juin 2024",
          location: "Belfort",
          description:
            "Conception et mise en œuvre des modèles d’apprentissage fédéré adaptés à l’optimisation de la consommation et de la production d’énergie solaire et éolienne ",
          color: "#2a8fff",
        },
      ],
    },
    {
      title: "Evénementiel",
      experiences: [
        {
          title: "Game Jam 2024",
          company: "Universite de Franche Comté",
          company_url: "",
          logo_path: "gamejam.jpg",
          duration: "48h",
          location: "Pavillions des sciences",
          description:
            "Participation à la conception et développement du jeu multijoueu Pimpon en 48h, où des pompiers traquent un pyromane caché parmi eux, similaire à Among Us.",
          color: "#24c029",
        },
        {
          title: "Hackathon 2024",
          company: "IUT Nord Franche-Comté",
          logo_path: "iut.jpeg",
          duration: "24h",
          location: "Belfort, IUT Nord Franche-Comté",
          description:
            "J'ai participé à un hackathon où j'ai contribué à une équipe ayant obtenu la troisième place.",
          color: "#bf4e21",
        },
        {
          title: "Hackathon 2023",
          company: "IUT Nord Franche-Comté",
          logo_path: "iut.jpeg",
          duration: "24h",
          location: "Belfort, IUT Nord Franche-Comté",
          description:
            "J'ai participé à un hackathon où j'ai contribué à une équipe ayant obtenu la cinquième place.",
          color: "#bf4e21",
        },
      ],
    },
  ],
};

// Projects Page
const projectsHeader = {
  title: "Projets",
  description:
    "Explorez mes réalisations les plus récentes et découvrez mon travail créatif dans le développement d'application. Des sites web dynamiques aux applications mobiles interactives !",
  avatar_image_path: "projects_image.svg",
};

const projects = {
  data: [
    {
      name: "Apprentissage fédéré",
      url: "",
      description:
        "Conception et mise en œuvre des modèles MLP en apprentissage fédéré adaptés à l’optimisation de la consommation et de la production d’énergie solaire.",
      languages: [
        {
          name: "Python",
          iconifyClass: "devicon-python",
        },
        {
          name: "Jupyter",
          iconifyClass: "devicon:jupyter",
        },
        {
          name: "Matplotlib",
          iconifyClass: "devicon:matplotlib",
        },
        {
          name: "Scikit-Learn",
          iconifyClass: "devicon:scikitlearn",
        },
        {
          name: "Numpy",
          iconifyClass: "devicon:numpy",
        },
      ],
    },

    {
      name: "Jeu de labyrinthe",
      url: "",
      description:
        "Développement d'un jeu de labyrinthe en Python dans le monde de Minecraft, élu meilleur projet de tout le lycée.",
      languages: [
        {
          name: "Python",
          iconifyClass: "devicon-python",
        },
      ],
    },
    {
      name: "Site présentant Fortiche",
      url: "",
      description:
        "Développement d'un site internet pour présenter l'entreprise Fortiche.",
      languages: [
        {
          name: "HTML",
          iconifyClass: "devicon-html5",
        },
        {
          name: "CSS",
          iconifyClass: "devicon-css3",
        },
        {
          name: "JavaScript",
          iconifyClass: "devicon-javascript",
        },
      ],
    },
    {
      name: "StuckWin",
      url: "",
      description:
        "Développement d'un jeu de plateau en Java et StdDraw appelé 'StuckWin' jouable contre des IA .",
      languages: [
        {
          name: "Java",
          iconifyClass: "devicon-java",
        },
      ],
    },
    {
      name: "Meez",
      url: "",
      description:
        "Développement d'un site de e-commerce pour la vente de vélos.",
      languages: [
        {
          name: "Python",
          iconifyClass: "devicon-python",
        },
        {
          name: "Flask",
          iconifyClass: "devicon-flask",
        },
        {
          name: "MySQL",
          iconifyClass: "devicon-mysql",
        },
        {
          name: "Django",
          iconifyClass: "devicon-plain:django",
        },
        {
          name: "HTML",
          iconifyClass: "devicon-html5",
        },
        {
          name: "CSS",
          iconifyClass: "devicon-css3",
        },
      ],
    },
    {
      name: "Machine virtuelle",
      url: "",
      description:
        "Création et configuration d'une machine virtuelle pour un projet SAE, incluant l'installation et la configuration manuelle.",
      languages: [
        {
          name: "Virtualization",
          iconifyClass: "simple-icons:virtualbox",
          color: "#2a8fff",
        },
        {
          name: "Linux",
          iconifyClass: "devicon-linux",
        },
      ],
    },
    {
      name: "Pimpon GameJAM 2023",
      url: "",
      description:
        "Développement d'un jeu ou des pompiers traquent un pyromane caché parmi eux, similaire à Among Us.",
      languages: [
        {
          name: "JavaScript",
          iconifyClass: "devicon-javascript",
        },
        {
          name: "Socket.io",
          iconifyClass: "devicon:socketio",
          color: "#FFFFFF",
        },
        {
          name: "HTML",
          iconifyClass: "devicon-html5",
        },
        {
          name: "CSS",
          iconifyClass: "devicon-css3",
        },
      ],
    },
    {
      name: "Quoridor",
      url: "",
      description:
        "Création du jeu de plateau Quoridor avec une IA, en Java et JavaFx.",
      languages: [
        {
          name: "Java",
          iconifyClass: "devicon-java",
        },
      ],
    },
    {
      name: "Démineur",
      url: "",
      description: "Création d'un jeu de Démineur en C++ en QT5",
      languages: [
        {
          name: "C++",
          iconifyClass: "devicon:cplusplus",
        },
        {
          name: "QT5",
          iconifyClass: "devicon:qt",
        },
      ],
    },
    {
      name: "Simulation Tissu",
      url: "",
      description:
        "Développement d'une simulation de tissu en Python et d'une version JavaScript minifiée",
      languages: [
        {
          name: "Python",
          iconifyClass: "devicon-python",
        },
        {
          name: "Javascript",
          iconifyClass: "devicon-javascript",
        },
      ],
    },
    {
      name: "Belforaine",
      url: "https://codeky.fr",
      description:
        "Développement d'une application web full-stack pour la gestion de la manifestation Belforaine.",
      languages: [
        {
          name: "JavaScript",
          iconifyClass: "devicon-javascript",
        },
        {
          name: "Leaflet",
          iconifyClass: "logos-leaflet",
        },
        {
          name: "Vue.js",
          iconifyClass: "devicon-vuejs",
        },
        {
          name: "Express",
          iconifyClass: "devicon-express",
        },
        {
          name: "PostgreSQL",
          iconifyClass: "devicon-postgresql",
        },
      ],
    },
    {
      name: "Talky",
      url: "https://talky.codeky.fr",
      description:
        "Développement en cours d'une application de messagerie responsive. Inspiré de Discord",
      languages: [
        {
          name: "JavaScript",
          iconifyClass: "devicon-javascript",
        },
        {
          name: "Socket.io",
          iconifyClass: "devicon:socketio",
          color: "#FFFFFF",
        },
        {
          name: "Express",
          iconifyClass: "devicon-express",
        },
        {
          name: "Vue.js",
          iconifyClass: "devicon-vuejs",
        },
        {
          name: "PostgreSQL",
          iconifyClass: "devicon-postgresql",
        },
      ],
    },
  ],
};

const compentencesHeader = {
  title: "Competence",
  description:
    "Développement et illustration des compétences clés à travers mon stage",
  avatar_image_path: "projects_image.svg",
};
const compentences = {
  sections: [
    {
      title: "Compétence 4 : Gérer des données de l’information",
      corpus: [
        {
          type: "h1",
          content:
            "Mettre à jour et interroger une base de données relationnelle (AC 1) et Manipuler des données hétérogènes (AC 4)",
        },
        {
          type: "p",
          content:
            "La mise à jour et l'interrogation d'une base de données relationnelle sont essentielles pour gérer les données efficacement. Pendant mon stage, j'ai appliqué ces compétences pour manipuler les données météorologiques et énergétiques, en les structurant pour des analyses et des visualisations optimales.",
        },
        {
          type: "code",
          title: " Téléchargement et Prétraitement des Données",
          objectif:
            " Télécharger des données via une API, les organiser en un format structuré et nettoyé pour les intégrer dans des fichier csv.",
          code:
            "def download_data(url='api/wind-toolkit/v2/wind/wtk-download.csv', api_key=None,\n" +
            '                  email="zek.akburak@gmail.com", names=2009, wkt="POINT(-78.483215 32.104233)", directory=".",\n' +
            '                  filename="response.csv", **kwargs):\n' +
            '    url = f"http://developer.nrel.gov/{url}?api_key={api_key}&wkt={wkt}&email={email}&names={names}"\n' +
            "\n" +
            "    for key, value in kwargs.items():\n" +
            '        if key == "attributes":\n' +
            '            value = ",".join(value)\n' +
            '        url += f"&{key}={value}"\n' +
            '    response = requests.request("GET", url)\n' +
            "    if (response.status_code != 200):\n" +
            "        print(response.text)\n" +
            "        return\n" +
            "    if not os.path.exists(f'{directory}'):\n" +
            "        os.makedirs(f'{directory}')\n" +
            "    with open(f'{directory}/{filename}', 'wb') as f:\n" +
            "        f.write(response.content)\n" +
            "\n" +
            "    df_ref = pd.read_csv(f'{directory}/{filename}', skiprows=[0])\n" +
            "    df_ref['datetime'] = pd.to_datetime(df_ref[['Year', 'Month', 'Day', 'Hour', 'Minute']])\n" +
            "    df_ref.set_index('datetime', inplace=True)\n" +
            "    df_ref.drop(columns=['Year', 'Month', 'Day', 'Hour', 'Minute'], inplace=True)\n" +
            "    df_ref.dropna(axis=1, inplace=True)\n" +
            "    df_ref.drop_duplicates(inplace=True)\n" +
            "    df_ref.dropna(axis=0, inplace=True)\n" +
            "\n" +
            "    df_ref.to_csv(f'{directory}/{filename}', index=True)\n" +
            "    return df_ref\n",
          explication:
            "Ce script télécharge des données à partir d'une API, les nettoie et les organise en un format prêt à être utilisé pour le programme",
        },
        {
          type: "code",
          title: "Uniformisation des Colonnes des Données",
          objectif:
            "Assurer la cohérence des colonnes à travers différents fichiers de données pour faciliter l'analyse.",
          explication:
            "Cette fonction lit tous les fichiers CSV dans un répertoire, trouve les colonnes communes, et ajuste chaque fichier pour ne conserver que ces colonnes. Cela garantit que tous les fichiers ont une structure de données uniforme",
          code:
            "def uniform_columns(directory='.'):\n" +
            "    df_dict = {}\n" +
            "    for filename in os.listdir(directory):\n" +
            '        if filename.endswith(".csv"):\n' +
            "            df = pd.read_csv(f'{directory}/{filename}', parse_dates=True, index_col=0)\n" +
            "            df_dict[filename] = df\n" +
            "    # les colonnes en commun\n" +
            "    columns = set.intersection(*[set(df.columns) for df in df_dict.values()])\n" +
            "    for filename, df in df_dict.items():\n" +
            "        df = df[list(columns)].reindex(columns=columns)\n" +
            "        df.to_csv(f'{directory}/{filename}', index=True)\n" +
            '    print("Uniform columns")\n' +
            "            ",
        },
        {
          type: "code",
          title: "Groupement  des années en un fichier",
          objectif:
            "Organiser les fichiers de données par pays, facilitant l'analyse comparative.",
          explication:
            " Ce code regroupe les fichiers de données par pays en fonction des noms de fichiers, permettant une organisation plus structurée des données pour des analyses spécifiques par pays.",
          code:
            'def group_dataframes_by_country(directory="."):\n' +
            '    """\n' +
            "    Lit les fichiers CSV dans le répertoire spécifié et les groupe par pays.\n" +
            "    \n" +
            "    :param directory: Répertoire où se trouvent les fichiers CSV. Par défaut, le répertoire courant.\n" +
            "    :return: Un dictionnaire avec le nom du pays comme clé et une liste de dataframes comme valeur.\n" +
            '    """\n' +
            "    # Dictionnaire pour stocker les dataframes par pays\n" +
            "    dataframes_by_country = {}\n" +
            "\n" +
            "    # Obtenir la liste de tous les fichiers dans le répertoire spécifié\n" +
            '    files = [i for i in os.listdir(directory) if "AllYears" not in i ] \n' +
            "\n" +
            "\n" +
            "    # Lire chaque fichier CSV et le regrouper par pays\n" +
            "    for file in files:\n" +
            "        # Vérifier si le fichier est un CSV\n" +
            '        if file.endswith(".csv"):\n' +
            "            # Extraire le nom de fichier sans l'extension et les parenthèses\n" +
            "            filename = os.path.splitext(file)[0].split('(')[0].strip()\n" +
            "            \n" +
            "            # Si le pays n'est pas déjà une clé du dictionnaire, l'ajouter\n" +
            "            if filename not in dataframes_by_country:\n" +
            "                dataframes_by_country[filename] = []\n" +
            "            \n" +
            "            # Lire le fichier CSV et ajouter le dataframe à la liste correspondante\n" +
            "            filepath = os.path.join(directory, file)\n" +
            "            df = pd.read_csv(filepath, parse_dates=True, index_col=0)\n" +
            "            dataframes_by_country[filename].append(df)\n" +
            "\n" +
            "    return dataframes_by_country",
        },
        {
          type: "h1",
          content: "Optimiser les modèles de données de l’entreprise (AC 1)",
        },
        {
          type: "p",
          content:
            "L’optimisation des modèles de données implique l'amélioration des structures de données pour une performance accrue et une gestion efficace des informations. Pendant mon stage, j'ai travaillé sur le chargement et la préparation des données en utilisant des techniques de normalisation et de décalage temporel pour maximiser l’efficacité des modèles analytiques.",
        },
        {
          type: "code",
          title: "Chargement et Préparation des Données",
          objectif:
            " Charger et préparer les données en vue de l'entraînement de modèles, en intégrant des fonctionnalités comme le décalage des données, la normalisation, et la création de jeux de données pour l'entraînement, la validation et le test.",
          explication: [
            {
              type: "li",
              title: "Cette fonction permet de :",
              content: [
                "Charger les données à partir de fichiers CSV.",
                "Resampler les données selon une fréquence spécifiée.",
                "Décaler les colonnes pour créer des variables en décalé pour l'analyse temporelle.",
                "Normaliser les colonnes numériques pour améliorer l'entraînement des modèles.",
                "Diviser les données en jeux d'entraînement, de validation, et de test.",
              ],
            },
          ],
          code:
            "def load_data_per_client(y_column, x_columns=None, paths=None, resample='h', type_resample='mean', normalize=True,\n" +
            "                         test_size=0.3, random_state=None, shift_column=None, shift=None, no_normalize_col=None, y_normalize=False,\n" +
            '                         split=True, directory="."):\n' +
            "     #...\n" +
            "   \n" +
            "\n" +
            "    for path in progress:\n" +
            "       #...\n" +
            "        if type_resample == 'median':\n" +
            "            data = data.resample(resample).median()\n" +
            "        else:\n" +
            "            data = data.resample(resample).mean()\n" +
            "\n" +
            "        if shift_column:\n" +
            "            if not isinstance(shift_column, list):\n" +
            "                shift_column = [i for i in data.columns]\n" +
            "\n" +
            "            original_column = data[shift_column].copy()\n" +
            "            shifted_data_list = []\n" +
            "            for shift_hour in shift:\n" +
            "                shifted_data = original_column.shift(shift_hour)\n" +
            '                shifted_data.columns = [f"{column}_t{shift_hour * -1}_{resample}" for column in shifted_data.columns]\n' +
            "                shifted_data_list.append(shifted_data)\n" +
            "            drop_colonne = shift_column.copy()\n" +
            "            if y_column in shift_column:\n" +
            "                drop_colonne.remove(y_column)\n" +
            "            data = pd.concat([data] + shifted_data_list, axis=1)\n" +
            "            data.drop(columns=drop_colonne, inplace=True)\n" +
            "            \n" +
            "            \n" +
            "\n" +
            "        if normalize:\n" +
            "            numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns\n" +
            "\n" +
            "            if no_normalize_col:\n" +
            "                exclude_columns = no_normalize_col.copy()\n" +
            "                for col in no_normalize_col:\n" +
            '                    exclude_columns.extend([f"{col}_t{shift_hour * -1}_{resample}" for shift_hour in shift])\n' +
            "                numerical_columns = [col for col in numerical_columns if\n" +
            "                                     col not in exclude_columns or col not in no_normalize_col]\n" +
            "\n" +
            "            if y_column in numerical_columns:\n" +
            "                numerical_columns = numerical_columns.drop(y_column)\n" +
            "    \n" +
            "            xscaler = MinMaxScaler(feature_range=(0, 1))\n" +
            "            data[numerical_columns] = xscaler.fit_transform(data[numerical_columns])\n" +
            "            \n" +
            "            if y_normalize:\n" +
            "                yscaler = MinMaxScaler(feature_range=(0, 1))\n" +
            "                data[y_column] = yscaler.fit_transform(data[[y_column]])\n" +
            "                y_scale_list.append(yscaler)\n" +
            "                \n" +
            "        #...\n" +
            "\n" +
            "        if split:\n" +
            "\n" +
            "            if x_columns is not None:\n" +
            "                X = data[x_columns]\n" +
            "            else:\n" +
            "                X = data.drop(columns=[y_column])\n" +
            "            y = data[y_column]\n" +
            "\n" +
            "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n" +
            "            \n" +
            "        \n" +
            "\n" +
            "            _, X_val, _, y_val = train_test_split(X_train, y_train, test_size=test_size,\n" +
            "                                                              random_state=random_state)\n" +
            "\n" +
            "          #...\n" +
            "\n" +
            "        else:\n" +
            "            df_list.append(data[x_columns + [y_column]])\n" +
            "            shift_column = shift_original\n" +
            "\n" +
            "    if not split:\n" +
            "        return df_list\n" +
            "    if y_normalize:\n" +
            "        return X_train_list, X_val_list, X_test_list, y_train_list, y_val_list, y_test_list,y_scale_list\n" +
            "    \n" +
            "    return X_train_list, X_val_list, X_test_list, y_train_list, y_val_list, y_test_list",
        },
        {
          type: "h1",
          content: "Assurer la confidentialité des données (AC 2)",
        },
        {
          type: "p",
          content:
            "Assurer la confidentialité des données est crucial, en particulier dans les configurations de formation fédérée où les données ne doivent pas être partagées entre clients. J’a du utilisé l’apprentissage fédérée car c’est une technique d'apprentissage  qui permet de former un modèle global en utilisant des données distribuées sur plusieurs clients, sans centraliser les données sur un serveur et ainsi assure la confidentialité ",
        },
        {
          type: "img",
          filename: "fl.gif",
          descrption:
            "Les modèles locaux téléchargent le modèle global, s’entraînent avec leurs données locales, puis renvoient leurs mises à jour pour créer un modèle global amélioré.",
        },
        {
          type: "h1",
          content: "Visualisation des Données (AC 2)",
        },
        {
          type: "p",
          content:
            "La visualisation des données a été essentielle pour comprendre les tendances et les modèles des données. J'ai utilisé des bibliothèques telles que Matplotlib et Seabron pour créer des graphiques permettant de visualiser ces informations de manière claire et précise.",
        },

        {
          type: "img",
          filename: "variation.png",
          descrption:
            "Global Horizontal Irradiance (GHI) : La mesure de la densité de flux radiatif solaire totale en W/m² reçue par une surface horizontale mesuré par heure pour le 22 novembre 2023, montrant une courbe typique de l'irradiance solaire sur une journée.",
        },
        {
          type: "img",
          filename: "3d.png",
          descrption:
            "\n" +
            "Relation tridimensionnelle de l'Irradiance Globale Horizontale (GHI) mesurée à trois moments : GHI à l'instant t, GHI une heure plus tard (t+1), et GHI deux heures plus tard (t+2). Les points montrent la corrélation entre ces valeurs horaires.",
        },
        {
          type: "p",
          content:
            "Pendant mon stage, j'ai réalisé des dizaines de graphiques. Voici quelques exemples. Grâce à ce travail, j'ai amélioré ma compétence en visualisation de données en créant des graphiques clairs qui permettent d'interpréter et de communiquer les tendances et les anomalies, facilitant ainsi la prise de décision et la compréhension des données complexes.",
        },
      ],
    },
    {
      title: "Compétence 2 : Optimiser des applications",
      corpus: [
        {
          type: "h1",
          content: "Analyser un problème avec méthode",
        },
        {
          type: "code",
          title: "Découpage du code en élément simple",
          objectif:
            "Le code divise un algorithme en sous-algorithmes simples pour améliorer sa lisibilité et son efficacité. Cela rend le code plus clair et plus facile à comprendre, ce qui facilite le processus d'apprentissage fédéré pour entraîner des modèles de régression MLP sur plusieurs clients.",
          explication:
            "Chaque méthode d'agrégation est encapsulée dans une fonction distincte, telles que WeightedAggregatedModels, LaplacianMeanAggregatedModels, MedianAggregatedModels, TrimmedMeanAggregatedModels et GeometricMedianAggregatedModels, ainsi que les fonctions pour l'entraînement et le test des modèles. Ces fonctions permettent d'agréger de manière spécifique les poids et les biais des modèles provenant de différents clients, en fonction de la méthode choisie.Ces sous-algorithmes distincts permettent d'appliquer la compétence consistant à analyser un problème avec méthode, en découpant l'algorithme en éléments simples et en structurant les données de manière appropriée.",
          code:
            "def MLPTraining(X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test, hidden_layer_sizes=(64, 32),\n" +
            "  mlp_regressor=None, verbose=False, learning_rate_init=0.01, random_state=42, max_iter=10000,\n" +
            "  learning_rate=\"constant\", activation='relu', solver='adam'):\n" +
            "  #...\n" +
            "\n" +
            "\n" +
            "def MLPTesting(mlp_regressor_aggregated, X_val_scaled, y_val, X_test_scaled, y_test, verbose=False):\n" +
            "  #...\n" +
            "\n" +
            "def MeanAggregatedModels(WeightsAll, BiasesAll):\n" +
            "  #...\n" +
            "\n" +
            "def WeightedAggregatedModels(WeightsAll, BiasesAll, weights):\n" +
            "  #...\n" +
            "\n" +
            "def LaplacianMeanAggregatedModels(WeightsAll, BiasesAll, sensitivity, epsilon):\n" +
            "  #...\n" +
            "\n" +
            "\n" +
            "def MedianAggregatedModels(WeightsAll, BiasesAll):\n" +
            "  #...\n" +
            "\n" +
            "def TrimmedMeanAggregatedModels(WeightsAll, BiasesAll, trim_fraction):\n" +
            "  #...\n" +
            "\n" +
            "def GeometricMedianAggregatedModels(WeightsAll, BiasesAll):\n" +
            "  #...\n" +
            "\n" +
            "def federatedLearning(n_client, X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test,\n" +
            "  hidden_layer_sizes=(64, 32), round=5, aggregation='Mean', learning_rate_init=0.01,\n" +
            "  random_state=42, max_iter=10000, activation='relu', learning_rate=\"constant\", solver='adam', best_model=False,\n" +
            "  param_aggr=None):\n" +
            "\n" +
            "  #...\n" +
            "  for epoch in range(round):\n" +
            "    for iter in range(n_client):\n" +
            "      weights, biases, mlp_regressor = MLPTraining(X_train_scaled[iter], y_train[iter], X_val_scaled[iter],\n" +
            "        y_val[iter], X_test_scaled[iter], y_test[iter],\n" +
            "        hidden_layer_sizes=hidden_layer_sizes,\n" +
            "        learning_rate_init=learning_rate_init,\n" +
            "        learning_rate=learning_rate,\n" +
            "        random_state=random_state, max_iter=max_iter,\n" +
            "        activation=activation, solver=solver)\n" +
            "\n" +
            "  #...\n" +
            "  match aggregation:\n" +
            '     case "Mean":\n' +
            "       aggregatedWeights, aggregatedBiases = MeanAggregatedModels(WeightsAll, BiasesAll)\n" +
            '     case "Median":\n' +
            "       aggregatedWeights, aggregatedBiases = MedianAggregatedModels(WeightsAll, BiasesAll)\n" +
            '     case "Weighted":\n' +
            "       aggregatedWeights, aggregatedBiases = WeightedAggregatedModels(WeightsAll, BiasesAll, param_aggr)\n" +
            '     case "TrimmedMean":\n' +
            "       aggregatedWeights, aggregatedBiases = TrimmedMeanAggregatedModels(WeightsAll, BiasesAll, param_aggr)\n" +
            '     case "GeometricMedian":\n' +
            "       aggregatedWeights, aggregatedBiases = GeometricMedianAggregatedModels(WeightsAll, BiasesAll)\n" +
            '     case "Laplacian":\n' +
            "       aggregatedWeights, aggregatedBiases = LaplacianMeanAggregatedModels(WeightsAll, BiasesAll,\n" +
            "                                                                             sensitivity=param_aggr['sensitivity'],\n" +
            "                                                                             epsilon=param_aggr['epsilon'])\n" +
            "\n" +
            "  #...\n" +
            "temp_result = MLPTesting(mlp_regressor_aggregated, X_val_scaled[-1], y_val[-1], X_test_scaled[-1], y_test[-1])\n" +
            "\n" +
            "  #...\n",
        },
        {
          type: "h1",
          content: "Comparer des algorithmes pour des problèmes classiques ",
        },
        {
          type: "p",
          content:
            "La comparaison des algorithmes est essentielle pour déterminer lequel est le plus adapté à un problème donné. J'ai comparé plusieurs algorithmes de régression MLP pour l'apprentissage fédéré, tels que Mean, Median, Weighted, Trimmed Mean, Geometric Median et Laplacian Mean, pour évaluer leur performance et leur efficacité dans la formation de modèles de régression sur plusieurs clients.",
        },
        {
          type: "code",
          title: "Comparaison des Algorithmes de Régression MLP",
          objectif:
            "Comparer les performances des algorithmes de régression MLP pour l'apprentissage fédéré, en évaluant leur précision, leur vitesse d'apprentissage et leur capacité à généraliser les données.",
          explication:
            "Les algorithmes de régression MLP sont entraînés sur les données de chaque client, puis les modèles sont agrégés en utilisant différentes méthodes d'agrégation pour créer un modèle global. Ce modèle global est ensuite testé sur les données de validation et de test pour évaluer sa précision et sa capacité à généraliser les données. Les performances des algorithmes sont comparées en fonction de leur précision, de leur vitesse d'apprentissage et de leur capacité à généraliser les données, permettant de déterminer lequel est le plus adapté à un problème donné.",
          code:
            "def execute_ml_pipeline(y_column, paths, resample, resample_method, normalize, test_size, random_state,\n" +
            "                        shift, shift_column, no_normalize_col, N_CLIENT, hidden_layer_sizes, ROUND,\n" +
            "                        trim_fraction, sensitivity, epsilon, weight, activation, solver, learning_rate_init,\n" +
            "                        learning_rate, max_iter, aggregation_methods, directory, data_directory, x_columns=None,y_normalize=False):\n" +
            "    \n" +
            "    \n" +
            "    y_scaler_list = []\n" +
            "    \n" +
            "    if not y_normalize:\n" +
            "        if x_columns is None and y_column is not None and no_normalize_col is not None:\n" +
            "            no_normalize_col += [y_column]\n" +
            "    \n" +
            "        X_train, X_val, X_test, y_train, y_val, y_test = load_data_per_client(y_column, x_columns=x_columns, paths=paths,\n" +
            "                                                                              resample=resample,\n" +
            "                                                                              type_resample=resample_method,\n" +
            "                                                                              normalize=normalize, test_size=test_size,\n" +
            "                                                                              random_state=random_state, shift=shift,\n" +
            "                                                                              shift_column=shift_column,\n" +
            "                                                                              directory=data_directory,\n" +
            "                                                                              no_normalize_col=no_normalize_col\n" +
            "                                                                              )\n" +
            "        \n" +
            "        \n" +
            "    else:\n" +
            "         X_train, X_val, X_test, y_train, y_val, y_test , y_scaler_list = load_data_per_client(y_column, x_columns=x_columns, paths=paths,\n" +
            "                                                                              resample=resample,\n" +
            "                                                                              type_resample=resample_method,\n" +
            "                                                                              normalize=normalize, test_size=test_size,\n" +
            "                                                                              random_state=random_state, shift=shift,\n" +
            "                                                                              shift_column=shift_column,\n" +
            "                                                                              directory=data_directory,\n" +
            "                                                                              no_normalize_col=no_normalize_col,\n" +
            "                                                                                y_normalize=y_normalize\n" +
            "                                                                              )\n" +
            "        \n" +
            "    \n" +
            "    \n" +
            "\n" +
            "    result, result_one_client = test_all_aggregation_methods(N_CLIENT, X_train, y_train, X_val, y_val, X_test, y_test,y_scaler=y_scaler_list,\n" +
            "                                                             hidden_layer_sizes=hidden_layer_sizes, round=ROUND,\n" +
            "                                                             trim_fraction=trim_fraction, sensitivity=sensitivity,\n" +
            "                                                             epsilon=epsilon, weight=weight, activation=activation,\n" +
            "                                                             solver=solver, learning_rate_init=learning_rate_init,\n" +
            "                                                             learning_rate=learning_rate, random_state=random_state,\n" +
            "                                                             max_iter=max_iter, aggregation_methods=aggregation_methods)\n" +
            "\n" +
            "    result.to_csv(f'{directory}/FMLR/{N_CLIENT}_clients_{ROUND}_rounds.csv')\n" +
            "    plot_multiple_lineplots(data=result,\n" +
            "                        x_col='Round',\n" +
            "                        y_col=['Validation RMSE', 'Test RMSE', 'R2 Score','R2 Score (Inverse Normalize)','Test RMSE (Inverse Normalize)'],\n" +
            "                        hue_col='Method',\n" +
            "                        style_col='Method',\n" +
            "                        markers=True,\n" +
            "                        log_scale=False,\n" +
            "                        xlim=[[0, ROUND], [0, ROUND], [0, ROUND], [0, ROUND], [0, ROUND], [0, ROUND]],\n" +
            "                        ylim=[None, None, [0.8, 1], [0.8, 1], None, None],\n" +
            "                        filename=f'{N_CLIENT}_clients_{ROUND}_rounds.png',\n" +
            "                        directory=directory + '/FMLR'\n" +
            "                        )\n" +
            "    return result\n",
        },
        {
          type: "img",
          filename: "r2.png",
          descrption:
            "Comparaison des scores R2 pour les différentes méthodes d'agrégation des modèles de régression MLP sur plusieurs clients, montrant la performance relative de chaque méthode.",
        },
        {
          type: "img",
          filename: "rmse.png",
          descrption:
            "Comparaison des erreurs quadratiques moyennes pour les différentes méthodes d'agrégation des modèles de régression MLP sur plusieurs clients, montrant la précision relative de chaque méthode.",
        },
        {
          type: "p",
          content:
            "Les graphiques ci-dessus montrent la comparaison des scores R2 et des erreurs quadratiques moyennes pour les différentes méthodes d'agrégation des modèles de régression MLP sur plusieurs clients. Ces graphiques permettent de visualiser la performance relative de chaque méthode et de déterminer laquelle est la plus adaptée à un problème donné. Grâce à cette analyse comparative, j'ai amélioré ma compétence en comparaison des algorithmes pour des problèmes classiques, en évaluant les performances des algorithmes et en déterminant lequel est le plus efficace pour un problème donné.",
        },
        {
          type: "h1",
          content:
            "Utiliser des techniques algorithmiques adaptées pour des problèmes",
        },
        {
          type: "p",
          content:
            "L'utilisation de techniques algorithmiques adaptées est essentielle pour résoudre efficacement les problèmes complexes. J'avais un probleme de complexité lors de la recherche d'hyperparamètres optimaux pour les modèles de régression MLP. J'ai utilisé l'optimisation bayésienne pour trouver les hyperparamètres optimaux, en utilisant la bibliothèque scikit-optimize pour minimiser la fonction de perte et trouver les hyperparamètres qui maximisent la précision du modèle. Au lieu de tester manuellement différents hyperparamètres en faisant des boucles imbriquées.",
        },
        {
          type: "code",
          title:
            "Optimisation des Hyperparamètres avec l'Optimisation Bayésienne",
          objectif:
            "Optimiser les hyperparamètres des modèles de régression MLP en utilisant l'optimisation bayésienne pour minimiser la fonction de perte et maximiser la précision du modèle.",
          explication:
            "Le code suivant sert a trouvé l'hyperparamètre optimal pour les modèles de régression MLP en utilisant l'optimisation bayésienne.",
          code:
            "# Function with constrained hyperparameters for stability\n" +
            "def tune_mlp_regression_bayes(\n" +
            "        X,\n" +
            "        y,\n" +
            "        test_size=0.2,\n" +
            "        random_state=42,\n" +
            "        max_neurons_per_layer=200,  # Constrain maximum neurons\n" +
            "        layer=3,\n" +
            "        max_iter=3000,  # Constrain maximum iterations\n" +
            '        activation_functions=["relu", "tanh"],\n' +
            "        learning_rates=[0.00001, 0.0001, 0.001, 0.01, 0.1],  # Constrain learning rates\n" +
            '        learning_rate_types=["constant", "adaptive"],\n' +
            '        solvers=["adam", "sgd"],\n' +
            "        n_iter=100,  # Reduce number of iterations for BayesSearchCV\n" +
            "):\n" +
            "    search_space = {\n" +
            '        "activation": Categorical(activation_functions),\n' +
            '        "learning_rate": Categorical(learning_rate_types),\n' +
            '        "learning_rate_init": Real(min(learning_rates), max(learning_rates), prior="log-uniform"),\n' +
            '        "max_iter": Integer(1500, max_iter),  # Maximum iterations\n' +
            "    }\n" +
            " #...\n" +
            "    model = MLPRegressorTuned()\n" +
            "    scoring = make_scorer(mean_squared_error, greater_is_better=False)\n" +
            "\n" +
            "    bayes_search = BayesSearchCV(\n" +
            "        model,\n" +
            "        search_space,\n" +
            "        n_iter=n_iter,\n" +
            "        scoring=scoring,\n" +
            "        n_jobs=-1,\n" +
            "        cv=3,\n" +
            "        verbose=1,\n" +
            "        random_state=random_state,\n" +
            "    )\n" +
            "\n" +
            "    X_train, X_test, y_train, y_test = train_test_split(\n" +
            "        X, y, test_size=test_size, random_state=random_state\n" +
            "    )\n" +
            "\n" +
            "    try:\n" +
            "        bayes_search.fit(X_train, y_train)\n" +
            "    except ValueError as e:\n" +
            '        raise ValueError(f"Error during Bayesian optimization: {e}")\n' +
            "    return bayes_search",
        },
      ],
    },
  ],
};

export {
  greeting,
  socialMediaLinks,
  skills,
  degrees,
  certifications,
  experience,
  projectsHeader,
  projects,
  compentencesHeader,
  compentences,
};
